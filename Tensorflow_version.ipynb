{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fregean/Tensorflow-ver-mixed-segdec-net/blob/main/Tensorflow_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f-NK4nhQNPp"
      },
      "source": [
        "## Tensorflowへ変換"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfqp3WuA6GiQ"
      },
      "source": [
        "KSDD\n",
        "\n",
        "画像データ：1408*512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V5jw6uaVQNkL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/14_ブラザー工業様/tensorflow_ver_mixed-segdec-net')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjF-XeLrWq7l"
      },
      "source": [
        "## models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "moLVQgeVGdv2"
      },
      "outputs": [],
      "source": [
        "# @title models.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import math\n",
        "\n",
        "BATCHNORM_TRACK_RUNNING_STATS = False\n",
        "BATCHNORM_MOVING_AVERAGE_DECAY = 0.9997\n",
        "\n",
        "class Conv2D_init(layers.Conv2D):\n",
        "    def __init__(self, filters, kernel_size, **kwargs):\n",
        "        super(Conv2D_init, self).__init__(filters, kernel_size, **kwargs)\n",
        "        self.kernel_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.01)\n",
        "        self.bias_initializer = 'zeros'\n",
        "\n",
        "class FeatureNorm(layers.Layer):\n",
        "    # PyTorch版のfeature_index=1をfeature_index=-1に変更（channel first→channel last）\n",
        "    def __init__(self, num_features, feature_index=-1, rank=4, reduce_dims=(2, 3), eps=0.001, include_bias=True):\n",
        "        super(FeatureNorm, self).__init__()\n",
        "        self.shape = [1] * rank\n",
        "        self.shape[feature_index] = num_features\n",
        "        self.reduce_dims = reduce_dims\n",
        "        self.scale = self.add_weight(name='scale', shape=self.shape, initializer='ones', trainable=True)\n",
        "        self.eps = eps\n",
        "\n",
        "        if include_bias:\n",
        "            self.bias = self.add_weight(name='bias', shape=self.shape, initializer='zeros', trainable=True)\n",
        "        else:\n",
        "            self.bias = 0\n",
        "\n",
        "    def call(self, features):\n",
        "        f_std = tf.math.reduce_std(features, axis=self.reduce_dims, keepdims=True)\n",
        "        f_mean = tf.math.reduce_mean(features, axis=self.reduce_dims, keepdims=True)\n",
        "        return self.scale * ((features - f_mean) / tf.sqrt(f_std + self.eps)) + self.bias\n",
        "\n",
        "def _conv_block(out_channels, kernel_size, padding):\n",
        "    return tf.keras.Sequential([\n",
        "        Conv2D_init(filters=out_channels, kernel_size=kernel_size, padding=padding, use_bias=False),\n",
        "        FeatureNorm(num_features=out_channels, eps=0.001),\n",
        "        layers.ReLU()\n",
        "    ])\n",
        "\n",
        "class SegDecNet(Model):\n",
        "    def __init__(self, device, input_width, input_height, input_channels):\n",
        "        super(SegDecNet, self).__init__()\n",
        "        if input_width % 8 != 0 or input_height % 8 != 0:\n",
        "            raise Exception(f\"Input size must be divisible by 8! width={input_width}, height={input_height}\")\n",
        "        self.input_width = input_width\n",
        "        self.input_height = input_height\n",
        "        self.input_channels = input_channels\n",
        "        self.volume = tf.keras.Sequential([\n",
        "            _conv_block(32, 5, 'same'), \n",
        "            layers.MaxPool2D(2),\n",
        "            _conv_block(64, 5, 'same'),\n",
        "            _conv_block(64, 5, 'same'),\n",
        "            _conv_block(64, 5, 'same'),\n",
        "            layers.MaxPool2D(2),\n",
        "            _conv_block(64, 5, 'same'),\n",
        "            _conv_block(64, 5, 'same'),\n",
        "            _conv_block(64, 5, 'same'),\n",
        "            _conv_block(64, 5, 'same'),\n",
        "            layers.MaxPool2D(2),\n",
        "            _conv_block(1024, 15, 'same')\n",
        "        ])\n",
        "\n",
        "        self.seg_mask = tf.keras.Sequential([\n",
        "            Conv2D_init(filters=1, kernel_size=1, use_bias=False),\n",
        "            FeatureNorm(num_features=1, eps=0.001)\n",
        "        ])\n",
        "\n",
        "        self.extractor = tf.keras.Sequential([\n",
        "            layers.MaxPool2D(pool_size=2),\n",
        "            _conv_block(8, 5, padding='same'),\n",
        "            layers.MaxPool2D(pool_size=2),\n",
        "            _conv_block(16, 5, padding='same'),\n",
        "            layers.MaxPool2D(pool_size=2),\n",
        "            _conv_block(32, 5, padding='same')\n",
        "        ])\n",
        "\n",
        "        self.global_max_pool_feat = layers.GlobalMaxPooling2D(keepdims=True)\n",
        "        self.global_avg_pool_feat = layers.GlobalAveragePooling2D(keepdims=True)\n",
        "        self.global_max_pool_seg = layers.MaxPooling2D(pool_size=(self.input_height // 8, self.input_width // 8))\n",
        "        self.global_avg_pool_seg = layers.AveragePooling2D(pool_size=(self.input_height // 8, self.input_width // 8))\n",
        "\n",
        "        self.fc = layers.Dense(units=1)\n",
        "\n",
        "        self.volume_lr_multiplier_layer = GradientMultiplyLayer()\n",
        "        self.glob_max_lr_multiplier_layer = GradientMultiplyLayer()\n",
        "        self.glob_avg_lr_multiplier_layer = GradientMultiplyLayer()\n",
        "\n",
        "    def set_gradient_multipliers(self, multiplier):\n",
        "        self.volume_lr_multiplier_mask = tf.ones((1,)) * multiplier\n",
        "        self.glob_max_lr_multiplier_mask = tf.ones((1,)) * multiplier\n",
        "        self.glob_avg_lr_multiplier_mask = tf.ones((1,)) * multiplier\n",
        "\n",
        "    def call(self, input, training=False):\n",
        "        volume = self.volume(input, training=training)\n",
        "        seg_mask = self.seg_mask(volume, training=training)\n",
        "\n",
        "        # 表示用\n",
        "        self.volume_ = volume\n",
        "        self.seg_mask_ = seg_mask\n",
        "\n",
        "        cat = tf.concat([volume, seg_mask], axis=-1)\n",
        "\n",
        "        cat = self.volume_lr_multiplier_layer(cat, self.volume_lr_multiplier_mask)\n",
        "\n",
        "        features = self.extractor(cat, training=training)\n",
        "        global_max_feat = self.global_max_pool_feat(features)\n",
        "        global_avg_feat = self.global_avg_pool_feat(features)\n",
        "        global_max_seg = self.global_max_pool_seg(seg_mask)\n",
        "        global_avg_seg = self.global_avg_pool_seg(seg_mask)\n",
        "\n",
        "        global_max_seg = self.glob_max_lr_multiplier_layer(global_max_seg, self.glob_max_lr_multiplier_mask)\n",
        "        global_avg_seg = self.glob_avg_lr_multiplier_layer(global_avg_seg, self.glob_avg_lr_multiplier_mask)\n",
        "        \n",
        "\n",
        "        fc_in = tf.concat([global_max_feat, global_avg_feat, global_max_seg, global_avg_seg], axis=-1)\n",
        "        fc_in = tf.reshape(fc_in, [fc_in.shape[0], -1])\n",
        "        prediction = self.fc(fc_in)\n",
        "\n",
        "        return prediction, seg_mask\n",
        "\n",
        "class GradientMultiplyLayer(layers.Layer):\n",
        "    def call(self, input, mask_bw):\n",
        "        return input * mask_bw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtD7O_JPLV0v"
      },
      "source": [
        "## End2End"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tmJXTAuPLWHg"
      },
      "outputs": [],
      "source": [
        "# @title end2end.py \n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "#from models import SegDecNet\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import utils\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from data.dataset_catalog import get_dataset\n",
        "import random\n",
        "import cv2\n",
        "from config import Config\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "LVL_ERROR = 10\n",
        "LVL_INFO = 5\n",
        "LVL_DEBUG = 1\n",
        "\n",
        "LOG = 1  # Will log all mesages with lvl greater than this\n",
        "SAVE_LOG = True\n",
        "\n",
        "WRITE_TENSORBOARD = False\n",
        "\n",
        "class End2End:\n",
        "    def __init__(self, cfg: Config):\n",
        "        self.cfg: Config = cfg\n",
        "        self.storage_path: str = os.path.join(self.cfg.RESULTS_PATH, self.cfg.DATASET)\n",
        "\n",
        "    def _log(self, message, lvl=LVL_INFO):\n",
        "        n_msg = f\"{self.run_name} {message}\"\n",
        "        if lvl >= LOG:\n",
        "            print(n_msg)\n",
        "\n",
        "    def train(self):\n",
        "        self._set_results_path()\n",
        "        self._create_results_dirs()\n",
        "        self.print_run_params()\n",
        "        if self.cfg.REPRODUCIBLE_RUN:\n",
        "            self._log(\"Reproducible run, fixing all seeds to:1337\", LVL_DEBUG)\n",
        "            np.random.seed(1337)\n",
        "            tf.random.set_seed(1337)\n",
        "            random.seed(1337)\n",
        "\n",
        "        device = self._get_device()\n",
        "        model = self._get_model()\n",
        "        self.optimizer = self._get_optimizer(model)\n",
        "        loss_seg, loss_dec = self._get_loss(True), self._get_loss(False)\n",
        "\n",
        "        train_loader = get_dataset(\"TRAIN\", self.cfg)\n",
        "        validation_loader = get_dataset(\"VAL\", self.cfg)\n",
        "\n",
        "        # get_dataset関数でKSDDDatasetのインスタンスを取得\n",
        "        # train_dataset = get_dataset(\"TRAIN\", self.cfg)\n",
        "        # validation_dataset = get_dataset(\"VAL\", self.cfg)\n",
        "\n",
        "        # tf.data.Datasetのインスタンスを作成\n",
        "        # train_loader = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        # validation_loader = validation_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "        tensorboard_writer = summary_ops_v2.create_file_writer(self.tensorboard_path) if WRITE_TENSORBOARD else None\n",
        "\n",
        "        train_results = self._train_model(device, model, train_loader, loss_seg, loss_dec, self.optimizer, validation_loader, tensorboard_writer)\n",
        "        self._save_train_results(train_results)\n",
        "        self._save_model(model, os.path.join(self.model_path, \"final_state.tf\"))\n",
        "\n",
        "        self.eval(model, device, self.cfg.SAVE_IMAGES, False, False)\n",
        "\n",
        "        self._save_params()\n",
        "\n",
        "    def eval(self, model, save_images, plot_seg, reload_final):\n",
        "        if reload_final:\n",
        "            model.load_weights(os.path.join(self.model_path, \"final_state_dict.tf\"))\n",
        "        test_loader = get_dataset(\"TEST\", self.cfg)\n",
        "        self.eval_model(model, test_loader, save_folder=self.outputs_path, save_images=save_images, is_validation=False, plot_seg=plot_seg)\n",
        "    \n",
        "    def training_iteration(self, data, model, criterion_seg, criterion_dec, optimizer, weight_loss_seg, weight_loss_dec, iter_index):\n",
        "        images, seg_masks, seg_loss_masks, is_segmented, _, _, _ = data\n",
        "\n",
        "        batch_size = self.cfg.BATCH_SIZE\n",
        "        memory_fit = self.cfg.MEMORY_FIT  # Not supported yet for >1\n",
        "\n",
        "        num_subiters = int(batch_size / memory_fit)\n",
        "        \n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        total_loss_seg = 0\n",
        "        total_loss_dec = 0\n",
        "\n",
        "        \n",
        "        for sub_iter in range(num_subiters):\n",
        "            with tf.GradientTape() as tape:    \n",
        "                images_ = images[sub_iter * memory_fit:(sub_iter + 1) * memory_fit, :, :, :]\n",
        "                seg_masks_ = seg_masks[sub_iter * memory_fit:(sub_iter + 1) * memory_fit, :, :, :]\n",
        "                seg_loss_masks_ = seg_loss_masks[sub_iter * memory_fit:(sub_iter + 1) * memory_fit, :, :, :]\n",
        "                is_pos_ = tf.reshape(tf.reduce_max(seg_masks_), (memory_fit, 1))\n",
        "\n",
        "                decision, output_seg_mask = model(images_, training=True)\n",
        "\n",
        "                if is_segmented[sub_iter]:\n",
        "                    if self.cfg.WEIGHTED_SEG_LOSS:\n",
        "\n",
        "                        loss_seg = tf.reduce_mean(criterion_seg(output_seg_mask, seg_masks_)[:,:,:,np.newaxis] * seg_loss_masks_)\n",
        "                         \n",
        "                    else:\n",
        "                        loss_seg = criterion_seg(output_seg_mask, seg_masks_)[:,:,:,np.newaxis]\n",
        "                    loss_dec = criterion_dec(decision, is_pos_)\n",
        "\n",
        "                    total_loss_seg += loss_seg#.numpy().item()\n",
        "                    total_loss_dec += loss_dec#.numpy().item()\n",
        "                    \n",
        "                    total_correct += tf.reduce_sum(tf.cast(tf.equal(decision.numpy() > 0.0, tf.cast(tf.cast(is_pos_.numpy(), dtype=np.int32), tf.bool)), dtype=np.float32))\n",
        "                    loss = weight_loss_seg * loss_seg + weight_loss_dec * loss_dec\n",
        "                else:\n",
        "                    loss_dec = criterion_dec(decision, is_pos_)\n",
        "                    total_loss_dec += loss_dec#.numpy().item()\n",
        "\n",
        "                    total_correct += tf.reduce_sum(tf.cast(tf.equal(decision.numpy() > 0.0, tf.cast(tf.cast(is_pos_.numpy(), dtype=np.int32), tf.bool)), dtype=np.float32))\n",
        "                    loss = weight_loss_dec * loss_dec\n",
        "\n",
        "                total_loss += loss#.numpy().item()\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))    \n",
        "        \n",
        "\n",
        "        return total_loss_seg, total_loss_dec, total_loss, total_correct\n",
        "\n",
        "\n",
        "    def _train_model(self, device, model, train_loader, criterion_seg, criterion_dec, optimizer, validation_set, tensorboard_writer):\n",
        "        losses = []\n",
        "        validation_data = []\n",
        "        max_validation = -1\n",
        "        validation_step = self.cfg.VALIDATION_N_EPOCHS\n",
        "\n",
        "        num_epochs = self.cfg.EPOCHS\n",
        "        \n",
        "        samples_per_epoch = tf.data.experimental.cardinality(train_loader).numpy() * self.cfg.BATCH_SIZE   \n",
        "\n",
        "        self.set_dec_gradient_multiplier(model, 0.0)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            self.print_epoch(epoch)\n",
        "\n",
        "            weight_loss_seg, weight_loss_dec = self.get_loss_weights(epoch)\n",
        "\n",
        "            dec_gradient_multiplier = self.get_dec_gradient_multiplier()\n",
        "            self.set_dec_gradient_multiplier(model, dec_gradient_multiplier)\n",
        "            \n",
        "            epoch_loss_seg, epoch_loss_dec, epoch_loss = 0, 0, 0\n",
        "            epoch_correct = 0\n",
        "\n",
        "            for iter_index, (data) in enumerate(train_loader):\n",
        "\n",
        "                curr_loss_seg, curr_loss_dec, curr_loss, correct = self.training_iteration(data, model,\n",
        "                                                                                        criterion_seg,\n",
        "                                                                                        criterion_dec,\n",
        "                                                                                        optimizer, weight_loss_seg,\n",
        "                                                                                        weight_loss_dec, iter_index)\n",
        "\n",
        "                epoch_loss_seg += curr_loss_seg\n",
        "                epoch_loss_dec += curr_loss_dec\n",
        "                epoch_loss += curr_loss\n",
        "\n",
        "                # loss表示\n",
        "                self.print_loss(epoch_loss_seg, epoch_loss_dec)\n",
        "\n",
        "                epoch_correct += correct\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "\n",
        "                self._save_model(model, os.path.join(self.model_path, f\"ep_{epoch:02}.tf\"))\n",
        "    \n",
        "            epoch_loss_seg = epoch_loss_seg / samples_per_epoch\n",
        "            epoch_loss_dec = epoch_loss_dec / samples_per_epoch\n",
        "            epoch_loss = epoch_loss / samples_per_epoch\n",
        "            losses.append((epoch_loss_seg, epoch_loss_dec, epoch_loss, epoch))\n",
        "\n",
        "            if self.cfg.VALIDATE and (epoch % validation_step == 0 or epoch == num_epochs - 1):\n",
        "                validation_ap, validation_accuracy = self.eval_model(model, validation_set, None, False, True, False)\n",
        "                validation_data.append((validation_ap, epoch))\n",
        "\n",
        "                if validation_ap > max_validation:\n",
        "                    max_validation = validation_ap\n",
        "                    self._save_model(model, os.path.join(self.model_path, \"best_state_dict.tf\")) # .h5\n",
        "\n",
        "        return losses, validation_data\n",
        "\n",
        "\n",
        "    def eval_model(self, model, eval_loader, save_folder, save_images, is_validation, plot_seg):\n",
        "\n",
        "        res = []\n",
        "        predictions, ground_truths = [], []\n",
        "\n",
        "        for data_point in eval_loader:\n",
        "\n",
        "            image, seg_mask, seg_loss_mask, _, sample_name, _, _ = data_point\n",
        "            is_pos = tf.reduce_max(seg_mask) > 0\n",
        "            prediction, pred_seg = model(image, training=False)\n",
        "\n",
        "            pred_seg = tf.nn.sigmoid(pred_seg)\n",
        "            prediction = tf.nn.sigmoid(prediction)\n",
        "\n",
        "            prediction = prediction.numpy().item()\n",
        "            image = image.numpy()\n",
        "            pred_seg = pred_seg.numpy()\n",
        "            seg_mask = seg_mask.numpy()\n",
        "\n",
        "            predictions.append(prediction)\n",
        "            ground_truths.append(is_pos)\n",
        "            res.append((prediction, None, None, is_pos, sample_name[0]))\n",
        "            if not is_validation and save_images:\n",
        "                # image saving code here\n",
        "                plt.imsave(f\"{save_folder}/{sample_name[0]}_prediction.png\", prediction[0, :, :, 0], cmap='gray')\n",
        "                plt.imsave(f\"{save_folder}/{sample_name[0]}_image.png\", image[0, :, :, 0], cmap='gray')\n",
        "                plt.imsave(f\"{save_folder}/{sample_name[0]}_pred_seg.png\", pred_seg[0, :, :, 0], cmap='gray')\n",
        "                plt.imsave(f\"{save_folder}/{sample_name[0]}_seg_mask.png\", seg_mask[0, :, :], cmap='gray')\n",
        "\n",
        "        if is_validation:\n",
        "            self.print_eval(ground_truths, predictions)\n",
        "            metrics = utils.get_metrics(np.array(ground_truths), np.array(predictions))\n",
        "            return metrics[\"AP\"], metrics[\"accuracy\"]\n",
        "            \n",
        "        else:\n",
        "            utils.evaluate_metrics(res, self.run_path, self.run_name)\n",
        "\n",
        "\n",
        "    def get_dec_gradient_multiplier(self):\n",
        "        if self.cfg.GRADIENT_ADJUSTMENT:\n",
        "            grad_m = 0\n",
        "        else:\n",
        "            grad_m = 1\n",
        "\n",
        "        return grad_m\n",
        "\n",
        "    def set_dec_gradient_multiplier(self, model, multiplier):\n",
        "        # This function is not applicable in TensorFlow as it is specific to PyTorch.\n",
        "        model.set_gradient_multipliers(multiplier)\n",
        "\n",
        "    def get_loss_weights(self, epoch):\n",
        "        total_epochs = float(self.cfg.EPOCHS)\n",
        "\n",
        "        if self.cfg.DYN_BALANCED_LOSS:\n",
        "            seg_loss_weight = 1 - (epoch / total_epochs)\n",
        "            dec_loss_weight = self.cfg.DELTA_CLS_LOSS * (epoch / total_epochs)\n",
        "        else:\n",
        "            seg_loss_weight = 1\n",
        "            dec_loss_weight = self.cfg.DELTA_CLS_LOSS\n",
        "\n",
        "        return tf.constant(seg_loss_weight), tf.constant(dec_loss_weight)\n",
        "\n",
        "    def reload_model(self, model, load_final=False):\n",
        "        if self.cfg.USE_BEST_MODEL:\n",
        "            path = os.path.join(self.model_path, \"best_state_dict.tf\")# .h5\n",
        "            model.load_weights(path)\n",
        "        elif load_final:\n",
        "            path = os.path.join(self.model_path, \"final_state_dict.tf\")# .h5\n",
        "            model.load_weights(path)\n",
        "\n",
        "    def _save_params(self):\n",
        "        params = self.cfg.get_as_dict()\n",
        "        params_lines = sorted(map(lambda e: e[0] + \":\" + str(e[1]) + \"\\n\", params.items()))\n",
        "        fname = os.path.join(self.run_path, \"run_params.txt\")\n",
        "        with open(fname, \"w+\") as f:\n",
        "            f.writelines(params_lines)\n",
        "\n",
        "    def _save_train_results(self, results):\n",
        "        losses, validation_data = results\n",
        "        ls, ld, l, le = map(list, zip(*losses))\n",
        "        plt.plot(le, l, label=\"Loss\", color=\"red\")\n",
        "        plt.plot(le, ls, label=\"Loss seg\")\n",
        "        plt.plot(le, ld, label=\"Loss dec\")\n",
        "        plt.ylim(bottom=0)\n",
        "        plt.grid()\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        if self.cfg.VALIDATE:\n",
        "            v, ve = map(list, zip(*validation_data))\n",
        "            plt.twinx()\n",
        "            plt.plot(ve, v, label=\"Validation AP\", color=\"Green\")\n",
        "            plt.ylim((0, 1))\n",
        "        plt.legend()\n",
        "        plt.savefig(os.path.join(self.run_path, \"loss_val\"), dpi=200)\n",
        "\n",
        "        df_loss = pd.DataFrame(data={\"loss_seg\": ls, \"loss_dec\": ld, \"loss\": l, \"epoch\": le})\n",
        "        df_loss.to_csv(os.path.join(self.run_path, \"losses.csv\"), index=False)\n",
        "\n",
        "        if self.cfg.VALIDATE:\n",
        "            df_loss = pd.DataFrame(data={\"validation_data\": v, \"epoch\": ve})\n",
        "            df_loss.to_csv(os.path.join(self.run_path, \"validation.csv\"), index=False)\n",
        "\n",
        "    def _save_model(self, model, output_name):\n",
        "        print(output_name)\n",
        "        if os.path.isfile(output_name):\n",
        "            os.remove(output_name)\n",
        "\n",
        "        model.save_weights(output_name)\n",
        "\n",
        "    def _get_optimizer(self, model):\n",
        "        return tf.keras.optimizers.SGD(learning_rate=self.cfg.LEARNING_RATE)\n",
        "\n",
        "    def _get_loss(self, is_seg):\n",
        "        reduction = tf.keras.losses.Reduction.NONE if self.cfg.WEIGHTED_SEG_LOSS and is_seg else tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
        "        return tf.keras.losses.BinaryCrossentropy(reduction=reduction)\n",
        "\n",
        "\n",
        "    def _get_device(self):\n",
        "        return tf.device(f\"/GPU:{self.cfg.GPU}\")\n",
        "\n",
        "    def _set_results_path(self):\n",
        "        self.run_name = f\"{self.cfg.RUN_NAME}_FOLD_{self.cfg.FOLD}\" if self.cfg.DATASET in [\"KSDD\", \"DAGM\"] else self.cfg.RUN_NAME\n",
        "\n",
        "        results_path = os.path.join(self.cfg.RESULTS_PATH, self.cfg.DATASET)\n",
        "        self.tensorboard_path = os.path.join(results_path, \"tensorboard\", self.run_name)\n",
        "\n",
        "        run_path = os.path.join(results_path, self.cfg.RUN_NAME)\n",
        "        if self.cfg.DATASET in [\"KSDD\", \"DAGM\"]:\n",
        "            run_path = os.path.join(run_path, f\"FOLD_{self.cfg.FOLD}\")\n",
        "\n",
        "        self._log(f\"Executing run with path {run_path}\")\n",
        "\n",
        "        self.run_path = run_path\n",
        "        self.model_path = os.path.join(run_path, \"models\")\n",
        "        self.outputs_path = os.path.join(run_path, \"test_outputs\")\n",
        "\n",
        "    def _create_results_dirs(self):\n",
        "        list(map(utils.create_folder, [self.run_path, self.model_path, self.outputs_path, ]))\n",
        "\n",
        "    def _get_model(self):\n",
        "        seg_net = SegDecNet(self._get_device(), self.cfg.INPUT_WIDTH, self.cfg.INPUT_HEIGHT, self.cfg.INPUT_CHANNELS)\n",
        "        return seg_net\n",
        "\n",
        "    def print_run_params(self):\n",
        "        for l in sorted(map(lambda e: e[0] + \":\" + str(e[1]) + \"\\n\", self.cfg.get_as_dict().items())):\n",
        "            k, v = l.split(\":\")\n",
        "            print(f\"{k:25s} : {str(v.strip())}\")\n",
        "\n",
        "    def print_epoch(self, epoch):\n",
        "        t_now = datetime.datetime.now().time()\n",
        "        print(f'epoch:{epoch}>{t_now}')\n",
        "\n",
        "    def print_eval(self, ground_truths, predictions):\n",
        "        print(f'predictions:{predictions}')\n",
        "\n",
        "    def print_loss(self, epoch_loss_seg, epoch_loss_dec):\n",
        "        print(f'epoch_loss_seg:{epoch_loss_seg}, epoch_loss_dec:{epoch_loss_dec}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpAoUBW54KEK"
      },
      "source": [
        "以下はpyファイルの方を用いる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziB1AzkybdXU"
      },
      "source": [
        "## train_net.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4iaq2dVd4004",
        "outputId": "c749252b-7b77-47d7-dbf1-21b2e94a1fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_NAME_FOLD_0 Executing run with path /content/drive/MyDrive/14_ブラザー工業様/tensorflow_ver_mixed-segdec-net/datasets/RESULTS/KSDD/RUN_NAME/FOLD_0\n",
            "BATCH_SIZE                : 1\n",
            "DATASET                   : KSDD\n",
            "DATASET_PATH              : /content/drive/MyDrive/14_ブラザー工業様/tensorflow_ver_mixed-segdec-net/datasets/KSDD\n",
            "DELTA_CLS_LOSS            : 0.01\n",
            "DILATE                    : 7\n",
            "DYN_BALANCED_LOSS         : True\n",
            "EPOCHS                    : 50\n",
            "FOLD                      : 0\n",
            "FREQUENCY_SAMPLING        : True\n",
            "GPU                       : 0\n",
            "GRADIENT_ADJUSTMENT       : False\n",
            "INPUT_CHANNELS            : 1\n",
            "INPUT_HEIGHT              : 1408\n",
            "INPUT_WIDTH               : 512\n",
            "LEARNING_RATE             : 1.0\n",
            "MEMORY_FIT                : 1\n",
            "NUM_SEGMENTED             : 33\n",
            "ON_DEMAND_READ            : False\n",
            "REPRODUCIBLE_RUN          : False\n",
            "RESULTS_PATH              : /content/drive/MyDrive/14_ブラザー工業様/tensorflow_ver_mixed-segdec-net/datasets/RESULTS\n",
            "SAVE_IMAGES               : True\n",
            "TRAIN_NUM                 : 33\n",
            "USE_BEST_MODEL            : False\n",
            "VALIDATE                  : True\n",
            "VALIDATE_ON_TEST          : True\n",
            "VALIDATION_N_EPOCHS       : 5\n",
            "WEIGHTED_SEG_LOSS         : True\n",
            "WEIGHTED_SEG_LOSS_MAX     : 1\n",
            "WEIGHTED_SEG_LOSS_P       : 2\n",
            "epoch:0>03:26:42.328178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f891f25b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f891f25b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_loss_seg:-1.6933138224928257e-10, epoch_loss_dec:-4.214300155639648\n",
            "epoch_loss_seg:-237.92904663085938, epoch_loss_dec:37.19872283935547\n",
            "epoch_loss_seg:-713.7871704101562, epoch_loss_dec:124.48521423339844\n",
            "epoch_loss_seg:-1427.5743408203125, epoch_loss_dec:255.6753387451172\n",
            "epoch_loss_seg:-2379.29052734375, epoch_loss_dec:430.4193115234375\n",
            "epoch_loss_seg:-3568.935791015625, epoch_loss_dec:649.6659545898438\n",
            "epoch_loss_seg:-4996.51025390625, epoch_loss_dec:913.7020263671875\n",
            "epoch_loss_seg:-6571.693359375, epoch_loss_dec:621.6930541992188\n",
            "epoch_loss_seg:-8462.240234375, epoch_loss_dec:829.6842651367188\n",
            "epoch_loss_seg:-10590.716796875, epoch_loss_dec:1082.5855712890625\n",
            "epoch_loss_seg:-12957.1220703125, epoch_loss_dec:1379.56787109375\n",
            "epoch_loss_seg:-15561.45703125, epoch_loss_dec:1722.5341796875\n",
            "epoch_loss_seg:-18253.78125, epoch_loss_dec:1352.767822265625\n",
            "epoch_loss_seg:-21321.32421875, epoch_loss_dec:1721.66455078125\n",
            "epoch_loss_seg:-24626.796875, epoch_loss_dec:2144.6728515625\n",
            "epoch_loss_seg:-28170.19921875, epoch_loss_dec:2610.345703125\n",
            "epoch_loss_seg:-31951.53125, epoch_loss_dec:3118.462646484375\n",
            "epoch_loss_seg:-35970.79296875, epoch_loss_dec:3673.966796875\n",
            "epoch_loss_seg:-40227.984375, epoch_loss_dec:4268.8046875\n",
            "epoch_loss_seg:-44723.1015625, epoch_loss_dec:4908.50634765625\n",
            "epoch_loss_seg:-49456.1484375, epoch_loss_dec:5594.662109375\n",
            "epoch_loss_seg:-54427.125, epoch_loss_dec:6343.55859375\n",
            "epoch_loss_seg:-59284.92578125, epoch_loss_dec:5584.5791015625\n",
            "epoch_loss_seg:-64715.2734375, epoch_loss_dec:4437.17578125\n",
            "epoch_loss_seg:-70383.546875, epoch_loss_dec:3420.390869140625\n",
            "epoch_loss_seg:-76289.75, epoch_loss_dec:2419.497802734375\n",
            "epoch_loss_seg:-82433.8828125, epoch_loss_dec:1404.677978515625\n",
            "epoch_loss_seg:-88815.9453125, epoch_loss_dec:470.05462646484375\n",
            "epoch_loss_seg:-95435.9375, epoch_loss_dec:-422.3922119140625\n",
            "epoch_loss_seg:-102293.859375, epoch_loss_dec:-1319.9874267578125\n",
            "epoch_loss_seg:-109389.7109375, epoch_loss_dec:-2120.133544921875\n",
            "epoch_loss_seg:-116723.4921875, epoch_loss_dec:-2872.489990234375\n",
            "epoch_loss_seg:-124295.203125, epoch_loss_dec:-3587.68798828125\n",
            "epoch_loss_seg:-131804.171875, epoch_loss_dec:-2917.6025390625\n",
            "epoch_loss_seg:-139841.40625, epoch_loss_dec:-6476.9169921875\n",
            "epoch_loss_seg:-148116.5625, epoch_loss_dec:-10268.3408203125\n",
            "epoch_loss_seg:-156629.640625, epoch_loss_dec:-13886.12109375\n",
            "epoch_loss_seg:-165380.65625, epoch_loss_dec:-17743.607421875\n",
            "epoch_loss_seg:-174369.59375, epoch_loss_dec:-21764.3046875\n",
            "epoch_loss_seg:-183596.46875, epoch_loss_dec:-25572.02734375\n",
            "epoch_loss_seg:-193061.265625, epoch_loss_dec:-28736.0625\n",
            "epoch_loss_seg:-202713.96875, epoch_loss_dec:-25633.0703125\n",
            "epoch_loss_seg:-212649.390625, epoch_loss_dec:-126121.9453125\n",
            "epoch_loss_seg:-222822.75, epoch_loss_dec:-233986.65625\n",
            "epoch_loss_seg:-233234.03125, epoch_loss_dec:-341337.125\n",
            "epoch_loss_seg:-243883.234375, epoch_loss_dec:-441412.96875\n",
            "epoch_loss_seg:-254770.375, epoch_loss_dec:-553251.6875\n",
            "epoch_loss_seg:-265895.4375, epoch_loss_dec:-664433.875\n",
            "epoch_loss_seg:-281833.625, epoch_loss_dec:-558362.0\n",
            "epoch_loss_seg:-293423.34375, epoch_loss_dec:-75495256.0\n",
            "epoch_loss_seg:-305251.78125, epoch_loss_dec:-146123456.0\n",
            "epoch_loss_seg:-317316.875, epoch_loss_dec:-219300912.0\n",
            "epoch_loss_seg:-329621.03125, epoch_loss_dec:-293731520.0\n",
            "epoch_loss_seg:-342162.40625, epoch_loss_dec:-366912832.0\n",
            "epoch_loss_seg:-354940.71875, epoch_loss_dec:-437795488.0\n",
            "epoch_loss_seg:-367959.875, epoch_loss_dec:-483312256.0\n",
            "epoch_loss_seg:-381214.8125, epoch_loss_dec:-556353728.0\n",
            "epoch_loss_seg:-394708.46875, epoch_loss_dec:-628767552.0\n",
            "epoch_loss_seg:-408439.3125, epoch_loss_dec:-702587520.0\n",
            "epoch_loss_seg:-422406.625, epoch_loss_dec:-776189952.0\n",
            "epoch_loss_seg:-436613.09375, epoch_loss_dec:-850425600.0\n",
            "epoch_loss_seg:-451057.5, epoch_loss_dec:-926416832.0\n",
            "epoch_loss_seg:-1433733.25, epoch_loss_dec:-853210496.0\n",
            "epoch_loss_seg:-52775112704.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775129088.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775145472.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775161856.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775178240.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775194624.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775211008.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775227392.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775243776.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775260160.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775276544.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775292928.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775309312.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775325696.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775342080.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775362560.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775378944.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775399424.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775419904.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775440384.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775460864.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775481344.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775501824.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775522304.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775542784.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775563264.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775583744.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775604224.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775624704.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775645184.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775665664.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775686144.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775706624.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775727104.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775747584.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775768064.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775792640.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775817216.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775841792.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775866368.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775890944.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775915520.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775940096.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775964672.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52775989248.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776013824.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776038400.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776062976.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776087552.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776112128.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776140800.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776169472.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776198144.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776226816.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776255488.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776284160.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776312832.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776341504.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776370176.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776398848.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776427520.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776456192.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776484864.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776513536.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776542208.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776570880.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776599552.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776632320.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776665088.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776697856.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776730624.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776763392.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776796160.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776828928.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776861696.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776894464.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776927232.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776960000.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52776992768.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777025536.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777058304.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777091072.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777123840.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777156608.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777189376.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777226240.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777263104.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777299968.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777336832.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777373696.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777410560.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777447424.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777484288.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777521152.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777558016.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777594880.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777631744.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777668608.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777705472.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777742336.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777779200.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777816064.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777857024.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777897984.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777938944.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52777975808.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778016768.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778057728.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778098688.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778135552.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778176512.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778217472.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778258432.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778295296.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778336256.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778377216.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778418176.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778459136.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778500096.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778545152.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778590208.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778631168.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778676224.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778721280.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778766336.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778811392.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778856448.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778901504.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778946560.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52778991616.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779036672.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779081728.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779126784.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779171840.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779216896.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779261952.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779307008.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779356160.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779405312.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779454464.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779503616.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779552768.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779601920.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779651072.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779700224.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779749376.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779798528.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779847680.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779896832.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779945984.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52779995136.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780044288.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780093440.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780142592.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780195840.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780249088.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780302336.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780355584.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780408832.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780462080.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780511232.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780564480.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780617728.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780666880.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780720128.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780773376.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780826624.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780879872.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780933120.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52780986368.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781039616.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781096960.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781150208.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781207552.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781264896.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781322240.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781379584.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781436928.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781494272.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781551616.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781608960.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781662208.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781719552.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781776896.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781834240.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781891584.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52781948928.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782006272.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782063616.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782125056.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782186496.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782243840.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782305280.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782366720.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782428160.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782489600.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782551040.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782612480.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782673920.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782735360.0, epoch_loss_dec:1564886302720.0\n",
            "epoch_loss_seg:-52782796800.0, epoch_loss_dec:1564886302720.0\n",
            "/content/drive/MyDrive/14_ブラザー工業様/tensorflow_ver_mixed-segdec-net/datasets/RESULTS/KSDD/RUN_NAME/FOLD_0/models/ep_00.tf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5f30db643685>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# ここを実行する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mend2end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-3cecae5ded76>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtensorboard_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_ops_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mWRITE_TENSORBOARD\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_seg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_train_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final_state.tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3cecae5ded76>\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, device, model, train_loader, criterion_seg, criterion_dec, optimizer, validation_set, tensorboard_writer)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALIDATE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalidation_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mvalidation_ap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_ap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3cecae5ded76>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(self, model, eval_loader, save_folder, save_images, is_validation, plot_seg)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;31m# nanになっているかどうか\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolume_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg_mask_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mpred_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_seg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'End2End' object has no attribute 'print_debug'"
          ]
        }
      ],
      "source": [
        "# Colab用args\n",
        "import argparse\n",
        "from config import Config\n",
        "\n",
        "class args():\n",
        "    def __init__(self):\n",
        "        self.GPU=0\n",
        "        self.RUN_NAME='RUN_NAME'\n",
        "        self.DATASET='KSDD'\n",
        "        self.DATASET_PATH='/content/drive/MyDrive/14_ブラザー工業様/tensorflow_ver_mixed-segdec-net/datasets/KSDD'\n",
        "        self.EPOCHS=50\n",
        "        self.LEARNING_RATE=1.0\n",
        "        self.DELTA_CLS_LOSS = 0.01\n",
        "        self.BATCH_SIZE=1\n",
        "        self.WEIGHTED_SEG_LOSS=True\n",
        "        self.WEIGHTED_SEG_LOSS_P=2\n",
        "        self.WEIGHTED_SEG_LOSS_MAX=1\n",
        "        self.DYN_BALANCED_LOSS=True\n",
        "        self.GRADIENT_ADJUSTMENT=False # Trueを変更\n",
        "        self.FREQUENCY_SAMPLING=True\n",
        "        self.FOLD=0\n",
        "        self.TRAIN_NUM=33\n",
        "        self.NUM_SEGMENTED=33\n",
        "        self.RESULTS_PATH = \"/content/drive/MyDrive/14_ブラザー工業様/tensorflow_ver_mixed-segdec-net/datasets/RESULTS\"\n",
        "        \n",
        "        self.VALIDATE = True\n",
        "        self.VALIDATE_ON_TEST = True\n",
        "        self.VALIDATION_N_EPOCHS = 5\n",
        "        self.USE_BEST_MODEL = False\n",
        "\n",
        "        self.ON_DEMAND_READ = False\n",
        "        self.REPRODUCIBLE_RUN = False\n",
        "        self.MEMORY_FIT = 1\n",
        "        self.SAVE_IMAGES = True\n",
        "        self.DILATE = 7\n",
        "\n",
        "args=args()\n",
        "configuration = Config()\n",
        "configuration.merge_from_args(args)\n",
        "configuration.init_extra()\n",
        "\n",
        "end2end = End2End(cfg=configuration)\n",
        "\n",
        "# ここを実行する\n",
        "end2end.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "output_name='/content/drive/MyDrive/14_ブラザー工業様/tensorflow_ver_mixed-segdec-net/datasets/RESULTS/KSDD/RUN_NAME/FOLD_0/models/best_state_dict.h5'\n",
        "\n",
        "os.remove(output_name)"
      ],
      "metadata": {
        "id": "rGyZEEeGrFqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82gZPyewzVdk"
      },
      "outputs": [],
      "source": [
        "def downsize(image: np.ndarray, downsize_factor: tuple = (8, 8)) -> np.ndarray:\n",
        "        img_t = tf.convert_to_tensor(np.expand_dims(image, 0 if len(image.shape) == 3 else (0, 1)).astype(np.float32))\n",
        "        pad_size = min(downsize_factor[0], img_t.shape[1] - 1, img_t.shape[2] - 1)\n",
        "        print(img_t.shape)\n",
        "        img_t = tf.pad(img_t, [[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], mode='REFLECT')\n",
        "        print(img_t.shape)\n",
        "        image_np = tf.nn.avg_pool(img_t, ksize=(2 * pad_size + 1, 2 * pad_size + 1), strides=downsize_factor, padding='VALID')\n",
        "        print(image_np[0,0].shape)\n",
        "        return image_np[0] if len(image.shape) == 3 else image_np[0, 0]\n",
        "\n",
        "def to_tensor(x):\n",
        "        if x.dtype != np.float32:\n",
        "            x = (x / 255.0).astype(np.float32)   \n",
        "\n",
        "        if len(x.shape) != 3:\n",
        "            x = np.expand_dims(x, axis=-1)\n",
        "\n",
        "        x = tf.convert_to_tensor(x)\n",
        "        return x\n",
        "\n",
        "im = np.arange((720896))\n",
        "im = np.reshape(im, (1408, 512))\n",
        " \n",
        "seg_im = downsize(to_tensor(im), downsize_factor=(8,8))[np.newaxis,:,:,:]\n",
        "seg_im.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYdKssZvGTbx"
      },
      "outputs": [],
      "source": [
        "tf.reshape(tf.reduce_max(seg_im), dtype=tf.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYSRA9ssVEDb"
      },
      "outputs": [],
      "source": [
        "is_p = tf.cast(tf.reshape(tf.reduce_max(seg_im), (1, 1)), dtype=tf.bool)\n",
        "is_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJkE622QGe1P"
      },
      "outputs": [],
      "source": [
        "is_p = tf.reshape(tf.reduce_max(seg_im), (1, 1))\n",
        "is_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-DQ4pLpmjHt"
      },
      "outputs": [],
      "source": [
        "decision = [[[[0.]]]]\n",
        "tf.equal(decision > 0.0, is_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfJa7u_ECJqi"
      },
      "outputs": [],
      "source": [
        "decision = 1\n",
        "tf.reduce_sum(tf.cast(tf.equal(decision > 0.0, is_p), tf.float32)).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6cRaHxVvhZN"
      },
      "outputs": [],
      "source": [
        "def downsize(image: np.ndarray, downsize_factor: int = 8) -> np.ndarray:\n",
        "    img_t = tf.convert_to_tensor(np.expand_dims(image, 0 if len(image.shape) == 3 else (0, -1)).astype(np.float32))\n",
        "    print(img_t.shape)\n",
        "    pad_size = min(downsize_factor, img_t.shape[1] - 1, img_t.shape[2] - 1)\n",
        "    img_t = tf.pad(img_t, [[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]], mode='REFLECT')\n",
        "    print(img_t.shape)\n",
        "    image_np = tf.nn.avg_pool(img_t, ksize=2 * pad_size + 1, strides=downsize_factor, padding='VALID')\n",
        "    print(image_np[0,0].shape)\n",
        "    return image_np[0] if len(image.shape) == 3 else image_np[0, 0]\n",
        "\n",
        "im = np.zeros((720896))\n",
        "im = np.reshape(im, (1408, 512))\n",
        " \n",
        "to_tensor(downsize(im, downsize_factor=8)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CJlQBObyb1B"
      },
      "outputs": [],
      "source": [
        "im = np.zeros((720896))\n",
        "im = np.reshape(im, (1408, 512))\n",
        "\n",
        "img_t = tf.convert_to_tensor(np.expand_dims(im, -1 if len(im.shape) == 3 else (0, -1)))\n",
        "\n",
        "img_t.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Cwo4nSGbpE"
      },
      "outputs": [],
      "source": [
        "# ここからテスト--------------------------\n",
        "from data.input_ksdd import KSDDDataset\n",
        "from data.dataset import Dataset\n",
        "import pickle\n",
        "\n",
        "kind = \"TRAIN\"\n",
        "ds = KSDDDataset(configuration.DATASET_PATH, configuration, kind)\n",
        "dtst = Dataset(configuration.DATASET_PATH, configuration, kind)\n",
        "\n",
        "print(type(ds))\n",
        "\n",
        "shuffle = kind == \"TRAIN\"\n",
        "batch_size = configuration.BATCH_SIZE if kind == \"TRAIN\" else 1\n",
        "\n",
        "dataset = ds._data.shuffle(buffer_size=ds.length)\n",
        "dataset = dataset.batch(batch_size)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "train_num =33\n",
        "num_segmented =33\n",
        "fold = 0\n",
        "\n",
        "fn = f\"splits/KSDD/split_{train_num}_{num_segmented}.pyb\"\n",
        "with open(f\"{fn}\", \"rb\") as f:\n",
        "    train_samples, test_samples = pickle.load(f)\n",
        "    if kind == 'TRAIN':\n",
        "        folders = train_samples[fold]\n",
        "    elif kind == 'TEST':\n",
        "        folders = test_samples[fold]\n",
        "    else:\n",
        "        raise Exception('Unknown')\n",
        "\n",
        "pos_samples, neg_samples = [], []\n",
        "for f, is_segmented in folders:\n",
        "    for sample in sorted(os.listdir(os.path.join(configuration.DATASET_PATH, f))):\n",
        "        if not sample.__contains__('label'):\n",
        "            image_path = configuration.DATASET_PATH + '/' + f + '/' + sample\n",
        "            seg_mask_path = f\"{image_path[:-4]}_label.bmp\"\n",
        "            image = dtst.read_img_resize(image_path, configuration.INPUT_CHANNELS, (configuration.INPUT_WIDTH, configuration.INPUT_HEIGHT))\n",
        "            seg_mask, positive = dtst.read_label_resize(seg_mask_path, (configuration.INPUT_WIDTH, configuration.INPUT_HEIGHT), dilate=configuration.DILATE)\n",
        "            sample_name = f\"{f}_{sample}\"[:-4]\n",
        "            if sample_name == 'kos21_Part7':\n",
        "                continue\n",
        "            if positive:\n",
        "                image = dtst.to_tensor(image)\n",
        "                seg_loss_mask = dtst.distance_transform(seg_mask, configuration.WEIGHTED_SEG_LOSS_MAX, configuration.WEIGHTED_SEG_LOSS_P)\n",
        "                seg_loss_mask = dtst.to_tensor(dtst.downsize(seg_loss_mask))\n",
        "                seg_mask = dtst.to_tensor(dtst.downsize(seg_mask))\n",
        "                pos_samples.append((image, seg_mask, seg_loss_mask, is_segmented, image_path, seg_mask_path, sample_name))\n",
        "            else:\n",
        "                image = dtst.to_tensor(image)\n",
        "                seg_loss_mask = dtst.to_tensor(dtst.downsize(np.ones_like(seg_mask)))\n",
        "                seg_mask = dtst.to_tensor(dtst.downsize(seg_mask))\n",
        "                neg_samples.append((image, seg_mask, seg_loss_mask, True, image_path, seg_mask_path, sample_name))\n",
        "\n",
        "# 長さ\n",
        "print(len(pos_samples)) # 34\n",
        "print(len(neg_samples)) # 230\n",
        "_data = pos_samples + neg_samples\n",
        "print(len(_data)) # 264\n",
        "\n",
        "# len(_data)の長さ取れてる！！！！！\n",
        "\n",
        "# ここまでテスト--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwegBYTPvF9T"
      },
      "outputs": [],
      "source": [
        "ds._data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1celz40wxd3"
      },
      "outputs": [],
      "source": [
        "# データの準備\n",
        "data_1 = tf.constant([1.0, 2.0, 3.0])  # テンソルに変換可能なデータ\n",
        "data_2 = tf.constant([0.1, 0.2, 0.3])  # テンソルに変換可能なデータ\n",
        "data_3 = [True, False, True]  # ブール型のリスト\n",
        "data_4 = ['a', 'b', 'c']  # 文字列のリスト\n",
        "\n",
        "# データセットの作成\n",
        "dataset_1 = tf.data.Dataset.from_tensor_slices(data_1)\n",
        "dataset_2 = tf.data.Dataset.from_tensor_slices(data_2)\n",
        "dataset_3 = tf.data.Dataset.from_tensor_slices(data_3)\n",
        "dataset_4 = tf.data.Dataset.from_tensor_slices(data_4)\n",
        "\n",
        "# データセットを結合\n",
        "dataset = tf.data.Dataset.zip((dataset_1, dataset_2, dataset_3, dataset_4))\n",
        "\n",
        "# 結果を確認\n",
        "for data in dataset:\n",
        "    print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpHghC2DMeBu"
      },
      "outputs": [],
      "source": [
        "print(dataset_1)\n",
        "print(dataset_1+dataset_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7cPWX06nz3O"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(buffer_size=100)\n",
        "dataset = dataset.batch(33)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIeWT54bnjRy"
      },
      "outputs": [],
      "source": [
        "for data in dataset:\n",
        "\n",
        "    # 4つ\n",
        "    print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl4rm0QcnTP2"
      },
      "outputs": [],
      "source": [
        "tf.data.Dataset.prefetch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPkaZoao8vFk"
      },
      "source": [
        "# 以下はpyファイル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5ZU6zx3uaAQ"
      },
      "source": [
        "## evaluation.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir0R1vK_uaPi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import utils\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "\n",
        "def get_performance_eval(P, Y, names, data_dir, output_dir, folds, prefix='', thresholds_tups=None, save=True):\n",
        "    metrics = {}\n",
        "    precision_, recall_, thresholds = precision_recall_curve(Y.astype(np.int32), P)\n",
        "    metrics['precision'] = precision_\n",
        "    metrics['recall'] = recall_\n",
        "    metrics['thresholds'] = thresholds\n",
        "\n",
        "    FPR, TPR, _ = roc_curve(Y.astype(np.int32), P)\n",
        "    AUC = auc(FPR, TPR)\n",
        "    AP = average_precision_score(Y.astype(np.int32), P)\n",
        "    metrics['FPR'] = FPR\n",
        "    metrics['TPR'] = TPR\n",
        "    metrics['AUC'] = AUC\n",
        "\n",
        "    f_measures = 2 * (precision_ * recall_) / (precision_ + recall_ + 0.0000000001)\n",
        "    metrics['f_measures'] = f_measures\n",
        "\n",
        "    thresholds_metrics = {}\n",
        "    ix_best = np.argmax(f_measures)\n",
        "    if ix_best > 0:\n",
        "        best_threshold = (thresholds[ix_best] + thresholds[ix_best - 1]) / 2\n",
        "    else:\n",
        "        best_threshold = thresholds[ix_best]\n",
        "    fn0_threshold = thresholds[np.where(recall_ >= 1)][0]\n",
        "\n",
        "    for thresh, name, dir in zip([best_threshold, fn0_threshold, 0.5], ['best', 'fn0', '50_perc'], ['best', 'fn0', '50_perc']):\n",
        "        FN, FP, TN, TP = get_and_copy_falses(P, Y, thresh, data_dir, folds, names, os.path.join(output_dir, dir), prefix, save=save)\n",
        "        F_measure = (2 * TP.sum()) / float(2 * TP.sum() + FP.sum() + FN.sum())\n",
        "        thresholds_metrics[name] = {}\n",
        "        thresholds_metrics[name]['value'] = thresh\n",
        "        thresholds_metrics[name]['TP'] = TP.sum()\n",
        "        thresholds_metrics[name]['TN'] = TN.sum()\n",
        "        thresholds_metrics[name]['FP'] = FP.sum()\n",
        "        thresholds_metrics[name]['FN'] = FN.sum()\n",
        "        thresholds_metrics[name]['F_measure'] = F_measure\n",
        "\n",
        "    metrics['AP'] = AP\n",
        "    metrics['thresholds'] = thresholds_metrics\n",
        "\n",
        "    if save:\n",
        "        for thr, d in metrics['thresholds'].items():\n",
        "            print(f'THRESHOLD {prefix} {thr:>15} => VALUE={d[\"value\"]:.4f}, FP={d[\"FP\"]} FN={d[\"FN\"]}, AP={AP}')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def get_and_copy_falses(P, Y, best_threshold, data_dir, folds, names, output_dir, prefix, save=True):\n",
        "    FP, FN, TN, TP = utils.calc_confusion_mat(P >= best_threshold, Y)\n",
        "    # find FN and FP examples and copy them to folders\n",
        "    if save:\n",
        "        if not os.path.exists(output_dir):\n",
        "            utils.create_folder(output_dir)\n",
        "        FN_names = [(folds[i], names[i] + \".jpg\") for i in range(0, len(names)) if FN[i]]\n",
        "        FP_names = [(folds[i], names[i] + \".jpg\") for i in range(0, len(names)) if FP[i]]\n",
        "        copy_falses(FN_names, data_dir, output_dir, prefix)\n",
        "        copy_falses(FP_names, data_dir, output_dir, prefix, is_FN=False)\n",
        "    return FN, FP, TN, TP\n",
        "\n",
        "\n",
        "def copy_falses(names, data_dir, output_dir, prefix, is_FN=True):\n",
        "    for fold, n in names:\n",
        "        outputs_folder = os.path.join(data_dir, f'FOLD_{fold}', prefix + 'outputs')\n",
        "        f_name = list(filter(lambda s: s.endswith(n), os.listdir(outputs_folder)))\n",
        "        if len(f_name) > 0:\n",
        "            f_name = f_name[0]\n",
        "            acc = f_name[:5]\n",
        "            src_file = os.path.join(outputs_folder, f_name)\n",
        "            dst_file = os.path.join(output_dir, f'{\"FN\" if is_FN else \"FP\"}_{acc}_{f_name[6:]}')\n",
        "\n",
        "            try:\n",
        "                shutil.copy(src_file, dst_file)\n",
        "            except:\n",
        "                print(f\"error: cannot copy file {n}\")\n",
        "\n",
        "\n",
        "def evaluate_decision(run_dir, folds, ground_truth, img_names, predictions, prefix='', output_dir=None, thresholds=None, save=True):\n",
        "    if output_dir is None:\n",
        "        output_dir = run_dir\n",
        "\n",
        "    metrics = get_performance_eval(predictions, ground_truth, img_names, run_dir, output_dir, folds, prefix=prefix, thresholds_tups=thresholds, save=save)\n",
        "\n",
        "    best_tr_metrics = metrics['thresholds']['best']\n",
        "    tp_sum = best_tr_metrics['TP']\n",
        "    fp_sum = best_tr_metrics['FP']\n",
        "    fn_sum = best_tr_metrics['FN']\n",
        "    tn_sum = best_tr_metrics['TN']\n",
        "    AP = metrics['AP']\n",
        "    fp_0fn_sum = metrics['thresholds']['fn0']['FP']\n",
        "\n",
        "    if save:\n",
        "        print(f\"AP: {AP:.03f}, FP/FN: {fp_sum:d}/{fn_sum:d}, FP@FN=0: {fp_0fn_sum:d}\")\n",
        "\n",
        "        with open(os.path.join(output_dir, prefix + 'accuracy.txt'), 'w') as f:\n",
        "            f.write(f\"TP= {tp_sum}\\tFP={fp_sum}\\n\")\n",
        "            f.write(f\"FN= {fn_sum}\\tTN={tn_sum}\")\n",
        "\n",
        "        with open(os.path.join(output_dir, f'{prefix}metrics.pkl'), 'wb') as f:\n",
        "            pickle.dump(metrics, f)\n",
        "            f.close()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def evaluate_fold(results_folder, t_folds, t_gt, t_img_names, t_preds):\n",
        "    m_test = evaluate_decision(results_folder, t_folds, t_gt, t_img_names, t_preds, prefix='', output_dir=os.path.join(results_folder), save=False)\n",
        "\n",
        "    thresholds = m_test['thresholds']\n",
        "    best = thresholds['best']\n",
        "    t50 = thresholds['50_perc']\n",
        "    fn0 = thresholds['fn0']\n",
        "\n",
        "    cls_acc = (best[\"TP\"] + best[\"TN\"]) / (best[\"TP\"] + best[\"TN\"] + best[\"FP\"] + best[\"FN\"])\n",
        "    cls_acc_50 = (t50[\"TP\"] + t50[\"TN\"]) / (t50[\"TP\"] + t50[\"TN\"] + t50[\"FP\"] + t50[\"FN\"])\n",
        "\n",
        "    tpr = best[\"TP\"] / (best[\"TP\"] + best[\"FN\"])\n",
        "    tnr = best[\"TN\"] / (best[\"TN\"] + best[\"FP\"])\n",
        "\n",
        "    eval_res = {\"ap\": (m_test['AP']),\n",
        "                \"auc\": (m_test['AUC']),\n",
        "                \"fps\": (best['FP']),\n",
        "                \"fns\": (best['FN']),\n",
        "                \"best_t\": (best['value']),\n",
        "                \"t50_fps\": (t50['FP']),\n",
        "                \"t50_fns\": (t50['FN']),\n",
        "                \"fn0s\": (fn0['FP']),\n",
        "                \"fn0_t\": (fn0['value']),\n",
        "                \"f_measure\": (best[\"F_measure\"]),\n",
        "                \"cls_acc\": cls_acc,\n",
        "                \"f_measure_50\": t50[\"F_measure\"],\n",
        "                \"cls_acc_50\": cls_acc_50,\n",
        "                \"tpr\": tpr,\n",
        "                \"tnr\": tnr}\n",
        "    return eval_res\n",
        "\n",
        "\n",
        "def read_predictions(fold, prefix, run_dir):\n",
        "    predictions, decisions, ground_truth, img_names, folds = [], [], [], [], []\n",
        "    if fold is not None:\n",
        "        fold_path = os.path.join(run_dir, 'FOLD_{}'.format(fold), prefix + 'results.csv')\n",
        "        decisions, folds, ground_truth, img_names, predictions = read_directory(decisions, fold, fold_path, folds, ground_truth, img_names, predictions)\n",
        "    else:\n",
        "        results_path = os.path.join(run_dir, prefix + 'results.csv')\n",
        "        decisions, folds, ground_truth, img_names, predictions = read_directory(decisions, 0, results_path, folds, ground_truth, img_names, predictions)\n",
        "    img_names = list(map(str, img_names))\n",
        "    predictions, decisions, ground_truth, img_names, folds = list(map(np.array, [predictions, decisions, ground_truth, img_names, folds]))\n",
        "\n",
        "    valid_idx = (img_names != 'kos21_Part7')\n",
        "    predictions = predictions[valid_idx]\n",
        "    decisions = decisions[valid_idx]\n",
        "    ground_truth = ground_truth[valid_idx]\n",
        "    img_names = img_names[valid_idx]\n",
        "    folds = folds[valid_idx]\n",
        "\n",
        "    return decisions, folds, ground_truth, img_names, predictions\n",
        "\n",
        "\n",
        "def read_directory(decisions, f, fold_path, folds, ground_truth, img_names, predictions):\n",
        "    csv = pd.read_csv(fold_path)\n",
        "    n_samples_in_fold = len(list(csv['prediction']))\n",
        "    predictions = predictions + list(csv['prediction'])\n",
        "    decisions = decisions + list(csv['decision'])\n",
        "    ground_truth = ground_truth + list(csv['ground_truth'])\n",
        "    img_names = img_names + list(csv['img_name'])\n",
        "    folds = folds + ([f] * n_samples_in_fold)\n",
        "    return decisions, folds, ground_truth, img_names, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxMiVNz0sm3q"
      },
      "source": [
        "## config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofwr-piasnJc"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    GPU = None\n",
        "\n",
        "    RUN_NAME = None\n",
        "\n",
        "    DATASET = None  # KSDD, DAGM, STEEL, KSDD2\n",
        "    DATASET_PATH = None\n",
        "\n",
        "    EPOCHS = None\n",
        "\n",
        "    LEARNING_RATE = None\n",
        "    DELTA_CLS_LOSS = None\n",
        "\n",
        "    BATCH_SIZE = None\n",
        "\n",
        "    WEIGHTED_SEG_LOSS = None\n",
        "    WEIGHTED_SEG_LOSS_P = None\n",
        "    WEIGHTED_SEG_LOSS_MAX = None\n",
        "    DYN_BALANCED_LOSS = None\n",
        "    GRADIENT_ADJUSTMENT = None\n",
        "    FREQUENCY_SAMPLING = True\n",
        "\n",
        "    # Default values\n",
        "    FOLD = None\n",
        "    TRAIN_NUM = None\n",
        "    NUM_SEGMENTED = None\n",
        "    RESULTS_PATH = \"./RESULTS\" # TODO use when releasing\n",
        "    # RESULTS_PATH = \"/home/jakob/outputs/WEAKLY_LABELED/PC_DEBUG\" if \"CONTAINER_NODE\" in os.environ else \"/opt/workspace/host_storage_hdd/REWRITE/v2\"\n",
        "    SPLITS_PATH = None\n",
        "\n",
        "    VALIDATE = True\n",
        "    VALIDATE_ON_TEST = True\n",
        "    VALIDATION_N_EPOCHS = 5\n",
        "    USE_BEST_MODEL = False\n",
        "\n",
        "    ON_DEMAND_READ = False\n",
        "    REPRODUCIBLE_RUN = False\n",
        "    MEMORY_FIT = 1\n",
        "    SAVE_IMAGES = True\n",
        "    DILATE = 1\n",
        "\n",
        "    # Auto filled\n",
        "    INPUT_WIDTH = None\n",
        "    INPUT_HEIGHT = None\n",
        "    INPUT_CHANNELS = None\n",
        "\n",
        "    def init_extra(self):\n",
        "        if self.WEIGHTED_SEG_LOSS and (self.WEIGHTED_SEG_LOSS_P is None or self.WEIGHTED_SEG_LOSS_MAX is None):\n",
        "            raise Exception(\"You also need to specify p and scaling factor for weighted segmentation loss!\")\n",
        "        if self.NUM_SEGMENTED is None:\n",
        "            raise Exception(\"Missing NUM_SEGMENTED!\")\n",
        "        if self.DATASET == 'KSDD':\n",
        "            self.INPUT_WIDTH = 512\n",
        "            self.INPUT_HEIGHT = 1408\n",
        "            self.INPUT_CHANNELS = 1\n",
        "\n",
        "            if self.TRAIN_NUM is None:\n",
        "                raise Exception(\"Missing TRAIN_NUM for KSDD dataset!\")\n",
        "            if self.NUM_SEGMENTED is None:\n",
        "                raise Exception(\"Missing NUM_SEGMENTED for KSDD dataset!\")\n",
        "            if self.FOLD is None:\n",
        "                raise Exception(\"Missing FOLD for KSDD dataset!\")\n",
        "\n",
        "        elif self.DATASET == 'DAGM':\n",
        "            self.INPUT_WIDTH = 512\n",
        "            self.INPUT_HEIGHT = 512\n",
        "            self.INPUT_CHANNELS = 1\n",
        "            if self.NUM_SEGMENTED is None:\n",
        "                raise Exception(\"Missing NUM_SEGMENTED for DAGM dataset!\")\n",
        "            if self.FOLD is None:\n",
        "                raise Exception(\"Missing FOLD for DAGM dataset!\")\n",
        "        elif self.DATASET == 'STEEL':\n",
        "            self.INPUT_WIDTH = 1600\n",
        "            self.INPUT_HEIGHT = 256\n",
        "            self.INPUT_CHANNELS = 1\n",
        "\n",
        "            self.VALIDATE_ON_TEST = False\n",
        "            self.USE_BEST_MODEL = True\n",
        "            print(\"Will use best model according to validation loss, validation is not performed on test set!\")\n",
        "            if not self.ON_DEMAND_READ:\n",
        "                print(\"Will use ON_DEMAND_READ even though it is set on False!\")\n",
        "                self.ON_DEMAND_READ = True\n",
        "            if self.TRAIN_NUM is None:\n",
        "                raise Exception(\"Missing TRAIN_NUM for STEEL dataset!\")\n",
        "            if self.NUM_SEGMENTED is None:\n",
        "                raise Exception(\"Missing NUM_SEGMENTED for STEEL dataset!\")\n",
        "        elif self.DATASET == 'KSDD2':\n",
        "            self.INPUT_WIDTH = 232\n",
        "            self.INPUT_HEIGHT = 640\n",
        "            self.INPUT_CHANNELS = 3\n",
        "            if self.NUM_SEGMENTED is None:\n",
        "                raise Exception(\"Missing NUM_SEGMENTED for KSDD2 dataset!\")\n",
        "        else:\n",
        "            raise Exception('Unknown dataset {}'.format(self.DATASET))\n",
        "\n",
        "    def merge_from_args(self, args):\n",
        "        self.GPU = args.GPU\n",
        "        self.RUN_NAME = args.RUN_NAME\n",
        "        self.DATASET = args.DATASET\n",
        "        self.DATASET_PATH = args.DATASET_PATH\n",
        "        self.EPOCHS = args.EPOCHS\n",
        "        self.LEARNING_RATE = args.LEARNING_RATE\n",
        "        self.DELTA_CLS_LOSS = args.DELTA_CLS_LOSS\n",
        "        self.BATCH_SIZE = args.BATCH_SIZE\n",
        "        self.WEIGHTED_SEG_LOSS = args.WEIGHTED_SEG_LOSS\n",
        "        self.WEIGHTED_SEG_LOSS_P = args.WEIGHTED_SEG_LOSS_P\n",
        "        self.WEIGHTED_SEG_LOSS_MAX = args.WEIGHTED_SEG_LOSS_MAX\n",
        "        self.DYN_BALANCED_LOSS = args.DYN_BALANCED_LOSS\n",
        "        self.GRADIENT_ADJUSTMENT = args.GRADIENT_ADJUSTMENT\n",
        "        self.FREQUENCY_SAMPLING = args.FREQUENCY_SAMPLING\n",
        "        self.NUM_SEGMENTED = args.NUM_SEGMENTED\n",
        "\n",
        "        if args.FOLD is not None: self.FOLD = args.FOLD\n",
        "        if args.TRAIN_NUM is not None: self.TRAIN_NUM = args.TRAIN_NUM\n",
        "        if args.RESULTS_PATH is not None: self.RESULTS_PATH = args.RESULTS_PATH\n",
        "        if args.VALIDATE is not None: self.VALIDATE = args.VALIDATE\n",
        "        if args.VALIDATE_ON_TEST is not None: self.VALIDATE_ON_TEST = args.VALIDATE_ON_TEST\n",
        "        if args.VALIDATION_N_EPOCHS is not None: self.VALIDATION_N_EPOCHS = args.VALIDATION_N_EPOCHS\n",
        "        if args.USE_BEST_MODEL is not None: self.USE_BEST_MODEL = args.USE_BEST_MODEL\n",
        "        if args.ON_DEMAND_READ is not None: self.ON_DEMAND_READ = args.ON_DEMAND_READ\n",
        "        if args.REPRODUCIBLE_RUN is not None: self.REPRODUCIBLE_RUN = args.REPRODUCIBLE_RUN\n",
        "        if args.MEMORY_FIT is not None: self.MEMORY_FIT = args.MEMORY_FIT\n",
        "        if args.SAVE_IMAGES is not None: self.SAVE_IMAGES = args.SAVE_IMAGES\n",
        "        if args.DILATE is not None: self.DILATE = args.DILATE\n",
        "\n",
        "    def get_as_dict(self):\n",
        "        params = {\n",
        "            \"GPU\": self.GPU,\n",
        "            \"DATASET\": self.DATASET,\n",
        "            \"DATASET_PATH\": self.DATASET_PATH,\n",
        "            \"EPOCHS\": self.EPOCHS,\n",
        "            \"LEARNING_RATE\": self.LEARNING_RATE,\n",
        "            \"DELTA_CLS_LOSS\": self.DELTA_CLS_LOSS,\n",
        "            \"BATCH_SIZE\": self.BATCH_SIZE,\n",
        "            \"WEIGHTED_SEG_LOSS\": self.WEIGHTED_SEG_LOSS,\n",
        "            \"WEIGHTED_SEG_LOSS_P\": self.WEIGHTED_SEG_LOSS_P,\n",
        "            \"WEIGHTED_SEG_LOSS_MAX\": self.WEIGHTED_SEG_LOSS_MAX,\n",
        "            \"DYN_BALANCED_LOSS\": self.DYN_BALANCED_LOSS,\n",
        "            \"GRADIENT_ADJUSTMENT\": self.GRADIENT_ADJUSTMENT,\n",
        "            \"FREQUENCY_SAMPLING\": self.FREQUENCY_SAMPLING,\n",
        "            \"FOLD\": self.FOLD,\n",
        "            \"TRAIN_NUM\": self.TRAIN_NUM,\n",
        "            \"NUM_SEGMENTED\": self.NUM_SEGMENTED,\n",
        "            \"RESULTS_PATH\": self.RESULTS_PATH,\n",
        "            \"VALIDATE\": self.VALIDATE,\n",
        "            \"VALIDATE_ON_TEST\": self.VALIDATE_ON_TEST,\n",
        "            \"VALIDATION_N_EPOCHS\": self.VALIDATION_N_EPOCHS,\n",
        "            \"USE_BEST_MODEL\": self.USE_BEST_MODEL,\n",
        "            \"ON_DEMAND_READ\": self.ON_DEMAND_READ,\n",
        "            \"REPRODUCIBLE_RUN\": self.REPRODUCIBLE_RUN,\n",
        "            \"MEMORY_FIT\": self.MEMORY_FIT,\n",
        "            \"INPUT_WIDTH\": self.INPUT_WIDTH,\n",
        "            \"INPUT_HEIGHT\": self.INPUT_HEIGHT,\n",
        "            \"INPUT_CHANNELS\": self.INPUT_CHANNELS,\n",
        "            \"SAVE_IMAGES\": self.SAVE_IMAGES,\n",
        "            \"DILATE\": self.DILATE,\n",
        "        }\n",
        "        return params\n",
        "\n",
        "\n",
        "def load_from_dict(dictionary):\n",
        "    cfg = Config()\n",
        "\n",
        "    cfg.GPU = dictionary.get(\"GPU\", None)\n",
        "    cfg.DATASET = dictionary.get(\"DATASET\", None)\n",
        "    cfg.DATASET_PATH = dictionary.get(\"DATASET_PATH\", None)\n",
        "    cfg.EPOCHS = dictionary.get(\"EPOCHS\", None)\n",
        "    cfg.LEARNING_RATE = dictionary.get(\"LEARNING_RATE\", None)\n",
        "    cfg.DELTA_CLS_LOSS = dictionary.get(\"DELTA_CLS_LOSS\", None)\n",
        "    cfg.BATCH_SIZE = dictionary.get(\"BATCH_SIZE\", None)\n",
        "    cfg.WEIGHTED_SEG_LOSS = dictionary.get(\"WEIGHTED_SEG_LOSS\", None)\n",
        "    cfg.WEIGHTED_SEG_LOSS_P = dictionary.get(\"WEIGHTED_SEG_LOSS_P\", None)\n",
        "    cfg.WEIGHTED_SEG_LOSS_MAX = dictionary.get(\"WEIGHTED_SEG_LOSS_MAX\", None)\n",
        "    cfg.DYN_BALANCED_LOSS = dictionary.get(\"DYN_BALANCED_LOSS\", None)\n",
        "    cfg.GRADIENT_ADJUSTMENT = dictionary.get(\"GRADIENT_ADJUSTMENT\", None)\n",
        "    cfg.FREQUENCY_SAMPLING = dictionary.get(\"FREQUENCY_SAMPLING\", None)\n",
        "    cfg.FOLD = dictionary.get(\"FOLD\", None)\n",
        "    cfg.TRAIN_NUM = dictionary.get(\"TRAIN_NUM\", None)\n",
        "    cfg.NUM_SEGMENTED = dictionary.get(\"NUM_SEGMENTED\", None)\n",
        "    cfg.RESULTS_PATH = dictionary.get(\"RESULTS_PATH\", None)\n",
        "    cfg.VALIDATE = dictionary.get(\"VALIDATE\", None)\n",
        "    cfg.VALIDATE_ON_TEST = dictionary.get(\"VALIDATE_ON_TEST\", None)\n",
        "    cfg.VALIDATION_N_EPOCHS = dictionary.get(\"VALIDATION_N_EPOCHS\", None)\n",
        "    cfg.USE_BEST_MODEL = dictionary.get(\"USE_BEST_MODEL\", None)\n",
        "    cfg.ON_DEMAND_READ = dictionary.get(\"ON_DEMAND_READ\", None)\n",
        "    cfg.REPRODUCIBLE_RUN = dictionary.get(\"REPRODUCIBLE_RUN\", None)\n",
        "    cfg.MEMORY_FIT = dictionary.get(\"MEMORY_FIT\", None)\n",
        "    cfg.INPUT_WIDTH = dictionary.get(\"INPUT_WIDTH\", None)\n",
        "    cfg.INPUT_HEIGHT = dictionary.get(\"INPUT_HEIGHT\", None)\n",
        "    cfg.INPUT_CHANNELS = dictionary.get(\"INPUT_CHANNELS\", None)\n",
        "    cfg.SAVE_IMAGES = dictionary.get(\"SAVE_IMAGES\", None)\n",
        "    cfg.DILATE = dictionary.get(\"DILATE\", None)\n",
        "\n",
        "    return cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOSEPn6ot77v"
      },
      "source": [
        "## utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YVxiY_nt8Yj"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
        "import pandas as pd\n",
        "import os\n",
        "import errno\n",
        "import pickle\n",
        "import cv2\n",
        "\n",
        "\n",
        "def create_folder(folder, exist_ok=True):\n",
        "    try:\n",
        "        os.makedirs(folder)\n",
        "    except OSError as e:\n",
        "        if e.errno != errno.EEXIST or not exist_ok:\n",
        "            raise\n",
        "\n",
        "\n",
        "def calc_confusion_mat(D, Y):\n",
        "    FP = (D != Y) & (Y.astype(np.bool) == False)\n",
        "    FN = (D != Y) & (Y.astype(np.bool) == True)\n",
        "    TN = (D == Y) & (Y.astype(np.bool) == False)\n",
        "    TP = (D == Y) & (Y.astype(np.bool) == True)\n",
        "\n",
        "    return FP, FN, TN, TP\n",
        "\n",
        "\n",
        "def plot_sample(image_name, image, segmentation, label, save_dir, decision=None, blur=True, plot_seg=False):\n",
        "    plt.figure()\n",
        "    plt.clf()\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title('Input image')\n",
        "    if image.shape[0] < image.shape[1]:\n",
        "        image = np.transpose(image, axes=[1, 0, 2])\n",
        "        segmentation = np.transpose(segmentation)\n",
        "        label = np.transpose(label)\n",
        "    if image.shape[2] == 1:\n",
        "        plt.imshow(image, cmap=\"gray\")\n",
        "    else:\n",
        "        plt.imshow(image)\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title('Groundtruth')\n",
        "    plt.imshow(label, cmap=\"gray\")\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    if decision is None:\n",
        "        plt.title('Output')\n",
        "    else:\n",
        "        plt.title(f\"Output: {decision:.5f}\")\n",
        "    # display max\n",
        "    vmax_value = max(1, np.max(segmentation))\n",
        "    plt.imshow(segmentation, cmap=\"jet\", vmax=vmax_value)\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title('Output scaled')\n",
        "    if blur:\n",
        "        normed = segmentation / segmentation.max()\n",
        "        blured = cv2.blur(normed, (32, 32))\n",
        "        plt.imshow((blured / blured.max() * 255).astype(np.uint8), cmap=\"jet\")\n",
        "    else:\n",
        "        plt.imshow((segmentation / segmentation.max() * 255).astype(np.uint8), cmap=\"jet\")\n",
        "\n",
        "    out_prefix = '{:.3f}_'.format(decision) if decision is not None else ''\n",
        "\n",
        "    plt.savefig(f\"{save_dir}/{out_prefix}result_{image_name}.jpg\", bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    if plot_seg:\n",
        "        jet_seg = cv2.applyColorMap((segmentation * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "        cv2.imwrite(f\"{save_dir}/{out_prefix}_segmentation_{image_name}.png\", jet_seg)\n",
        "\n",
        "\n",
        "def evaluate_metrics(samples, results_path, run_name):\n",
        "    samples = np.array(samples)\n",
        "\n",
        "    img_names = samples[:, 4]\n",
        "    predictions = samples[:, 0]\n",
        "    labels = samples[:, 3].astype(np.float32)\n",
        "\n",
        "    metrics = get_metrics(labels, predictions)\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        data={'prediction': predictions,\n",
        "              'decision': metrics['decisions'],\n",
        "              'ground_truth': labels,\n",
        "              'img_name': img_names})\n",
        "    df.to_csv(os.path.join(results_path, 'results.csv'), index=False)\n",
        "\n",
        "    print(\n",
        "        f'{run_name} EVAL AUC={metrics[\"AUC\"]:f}, and AP={metrics[\"AP\"]:f}, w/ best thr={metrics[\"best_thr\"]:f} at f-m={metrics[\"best_f_measure\"]:.3f} and FP={sum(metrics[\"FP\"]):d}, FN={sum(metrics[\"FN\"]):d}')\n",
        "\n",
        "    with open(os.path.join(results_path, 'metrics.pkl'), 'wb') as f:\n",
        "        pickle.dump(metrics, f)\n",
        "        f.close()\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.clf()\n",
        "    plt.plot(metrics['recall'], metrics['precision'])\n",
        "    plt.title('Average Precision=%.4f' % metrics['AP'])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.savefig(f\"{results_path}/precision-recall.pdf\", bbox_inches='tight')\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.clf()\n",
        "    plt.plot(metrics['FPR'], metrics['TPR'])\n",
        "    plt.title('AUC=%.4f' % metrics['AUC'])\n",
        "    plt.xlabel('False positive rate')\n",
        "    plt.ylabel('True positive rate')\n",
        "    plt.savefig(f\"{results_path}/ROC.pdf\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "def get_metrics(labels, predictions):\n",
        "    metrics = {}\n",
        "    precision, recall, thresholds = precision_recall_curve(labels, predictions)\n",
        "    metrics['precision'] = precision\n",
        "    metrics['recall'] = recall\n",
        "    metrics['thresholds'] = thresholds\n",
        "    f_measures = 2 * np.multiply(recall, precision) / (recall + precision + 1e-8)\n",
        "    metrics['f_measures'] = f_measures\n",
        "    ix_best = np.argmax(f_measures)\n",
        "    metrics['ix_best'] = ix_best\n",
        "    best_f_measure = f_measures[ix_best]\n",
        "    metrics['best_f_measure'] = best_f_measure\n",
        "    best_thr = thresholds[ix_best]\n",
        "    metrics['best_thr'] = best_thr\n",
        "    FPR, TPR, _ = roc_curve(labels, predictions)\n",
        "    metrics['FPR'] = FPR\n",
        "    metrics['TPR'] = TPR\n",
        "    AUC = auc(FPR, TPR)\n",
        "    metrics['AUC'] = AUC\n",
        "    AP = auc(recall, precision)\n",
        "    metrics['AP'] = AP\n",
        "    decisions = predictions >= best_thr\n",
        "    metrics['decisions'] = decisions\n",
        "    FP, FN, TN, TP = calc_confusion_mat(decisions, labels)\n",
        "    metrics['FP'] = FP\n",
        "    metrics['FN'] = FN\n",
        "    metrics['TN'] = TN\n",
        "    metrics['TP'] = TP\n",
        "    metrics['accuracy'] = (sum(TP) + sum(TN)) / (sum(TP) + sum(TN) + sum(FP) + sum(FN))\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbXWzW1juO7D"
      },
      "source": [
        "## read_results.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ_vFYqjuPWr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import evaluation\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "from config import load_from_dict\n",
        "\n",
        "def get_params(cfg):\n",
        "    params_lines = []\n",
        "    for k, v in sorted(cfg.get_as_dict().items(), key=lambda x: x[0]):\n",
        "        params_lines.append(f'{k}:{v}')\n",
        "    return ','.join(params_lines)\n",
        "\n",
        "\n",
        "def get_run_config(run_path, fold):\n",
        "    params = {}\n",
        "    if fold is not None:\n",
        "        params_file = os.path.join(run_path, f'FOLD_{fold}', 'run_params.txt')\n",
        "    else:\n",
        "        params_file = os.path.join(run_path, 'run_params.txt')\n",
        "    with open(params_file, 'r') as f:\n",
        "        for l in f.readlines():\n",
        "            k, v = l.split(':')\n",
        "            params[k] = v.strip()\n",
        "    return load_from_dict(params)\n",
        "\n",
        "\n",
        "def read_results(results_path, dataset, folds=None, dagm_join=False, sortkey=itemgetter(0)):\n",
        "    results = []\n",
        "    results_columns = ['RUN_NAME',\n",
        "                       'TN', \"N_SEG\",\n",
        "                       'W_SEG_LOSS', 'W_P', 'W_MAX',\n",
        "                       'FRQ_SMP', 'DYN_B_L', 'DELTA',\n",
        "                       'EPS', 'LR',\n",
        "                       'AUC', 'AP',\n",
        "                       'FP', 'FN', 'FALSES', 'THRESH',\n",
        "                       \"F_MSR\", \"CLS_ACC\", \"TPR\", \"TNR\",\n",
        "                       '50_FP', '50_FN', '50_FALSES', '50_FMS', '50_CA',\n",
        "                       'FP@FN=0', 'THRESH@FN=0',\n",
        "                       'PATH', 'CONFIGURATION'\n",
        "                       ]\n",
        "    if dataset == \"DAGM\" and not dagm_join:\n",
        "        for f in folds:\n",
        "            process_dataset(results_path, dataset, [f], results, dagm_join)\n",
        "    else:\n",
        "        process_dataset(results_path, dataset, folds, results, dagm_join)\n",
        "\n",
        "    results = sorted(results, key=sortkey)\n",
        "    df = pd.DataFrame(results, columns=results_columns)\n",
        "    return df\n",
        "\n",
        "\n",
        "def process_dataset(results_path, dataset, folds, results, dagm_join):\n",
        "    for run_name in os.listdir(os.path.join(results_path, dataset)):\n",
        "        run_path = os.path.join(results_path, dataset, run_name)\n",
        "        try:\n",
        "            print(f\"Processing run_path: {run_path}\")\n",
        "            cfg = get_run_config(run_path, None if folds is None else folds[0])\n",
        "            ap, auc, fps, fns, t50_fps, t50_fns, fn0s, f_measure, cls_acc, f_measure_50, cls_acc_50, tpr, tnr = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
        "            best_t, fn0_t = -1, -1\n",
        "            for f in folds:\n",
        "                t_dec, t_folds, t_gt, t_img_names, t_preds = evaluation.read_predictions(f, '', run_path)\n",
        "                fold_eval_res = evaluation.evaluate_fold(run_path, t_folds, t_gt, t_img_names, t_preds)\n",
        "                if len(folds) == 1:\n",
        "                    best_t = fold_eval_res[\"best_t\"]\n",
        "                    fn0_t = fold_eval_res[\"fn0_t\"]\n",
        "                ap += fold_eval_res[\"ap\"]\n",
        "                auc += fold_eval_res[\"auc\"]\n",
        "                fps += fold_eval_res[\"fps\"]\n",
        "                fns += fold_eval_res[\"fns\"]\n",
        "                t50_fps += fold_eval_res[\"t50_fps\"]\n",
        "                t50_fns += fold_eval_res[\"t50_fns\"]\n",
        "                fn0s += fold_eval_res[\"fn0s\"]\n",
        "                f_measure += fold_eval_res[\"f_measure\"]\n",
        "                cls_acc += fold_eval_res[\"cls_acc\"]\n",
        "                f_measure_50 += fold_eval_res[\"f_measure_50\"]\n",
        "                cls_acc_50 += fold_eval_res[\"cls_acc_50\"]\n",
        "                tpr += fold_eval_res[\"tpr\"]\n",
        "                tnr += fold_eval_res[\"tnr\"]\n",
        "            ap /= len(folds)\n",
        "            auc /= len(folds)\n",
        "            f_measure /= len(folds)\n",
        "            cls_acc /= len(folds)\n",
        "            f_measure_50 /= len(folds)\n",
        "            cls_acc_50 /= len(folds)\n",
        "            tpr /= len(folds)\n",
        "            tnr /= len(folds)\n",
        "\n",
        "            if dataset == \"DAGM\" and not dagm_join:\n",
        "                run_name = f\"{run_name}_FOLD_{folds[0]}\"\n",
        "\n",
        "            results.append(\n",
        "                [run_name,\n",
        "                 cfg.TRAIN_NUM, cfg.NUM_SEGMENTED,\n",
        "                 cfg.WEIGHTED_SEG_LOSS, cfg.WEIGHTED_SEG_LOSS_P, cfg.WEIGHTED_SEG_LOSS_MAX,\n",
        "                 cfg.FREQUENCY_SAMPLING, cfg.DYN_BALANCED_LOSS, cfg.DELTA_CLS_LOSS,\n",
        "                 cfg.EPOCHS, cfg.LEARNING_RATE,\n",
        "                 f\"{auc:.5f}\", f\"{ap:.5f}\",\n",
        "                 fps, fns, fps + fns, f\"{best_t:.5f}\",\n",
        "                 f\"{f_measure:.5f}\", f\"{cls_acc:.5f}\", f\"{tpr:.5f}\", f\"{tnr:.5f}\",\n",
        "                 t50_fps, t50_fns, t50_fps + t50_fns, f\"{f_measure_50:.5f}\", f\"{cls_acc_50:.5f}\",\n",
        "                 fn0s, f\"{fn0_t:.5f}\",\n",
        "                 run_path, get_params(cfg)]\n",
        "            )\n",
        "\n",
        "        except Exception as f:\n",
        "            print(f'Error reading RUN {run_path} with Exception {f} ')\n",
        "\n",
        "\n",
        "def main():\n",
        "    # dataset,results_folder = \"STEEL\", '/home/jakob/outputs/WEAKLY_LABELED/STEEL/GRADIENT'\n",
        "    # dataset, results_folder = \"KSDD2\", '/home/jakob/outputs/WEAKLY_LABELED/KSDD2/GRADIENT'\n",
        "    # dataset, results_folder = \"DAGM\", '/home/jakob/outputs/WEAKLY_LABELED/DAGM/GS'\n",
        "    dataset, results_folder = \"KSDD\", '/home/jakob/outputs/WEAKLY_LABELED/RELEASE/'\n",
        "\n",
        "    dagm_join = False # If True will join(average) results for all classes\n",
        "\n",
        "    folds_dict = {\n",
        "        'KSDD': [0, 1, 2],\n",
        "        'KSDD2': [None],\n",
        "        'STEEL': [None],\n",
        "        'DAGM': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    }\n",
        "    results = read_results(results_folder, dataset, folds_dict[dataset], dagm_join, sortkey=itemgetter(0))\n",
        "    results.to_csv(os.path.join(results_folder, f'{dataset}_summary{f\"_joined\" if dataset == \"DAGM\" and dagm_join else \"\"}.csv'), index=False)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EROAd7UAWgDy"
      },
      "source": [
        "## demo_eval_single_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwOaZX_6ULYt"
      },
      "outputs": [],
      "source": [
        "from models import SegDecNet\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "INPUT_WIDTH = 512  # must be the same as it was during training\n",
        "INPUT_HEIGHT = 1408  # must be the same as it was during training\n",
        "INPUT_CHANNELS = 1  # must be the same as it was during training\n",
        "\n",
        "model = SegDecNet(INPUT_WIDTH, INPUT_HEIGHT, INPUT_CHANNELS)\n",
        "\n",
        "model_path = \"path_to_your_model\"\n",
        "model.load_weights(model_path)\n",
        "\n",
        "# %%\n",
        "img_path = \"path_to_the_test_image\"\n",
        "img = cv2.imread(img_path) if INPUT_CHANNELS == 3 else cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "img = cv2.resize(img, (INPUT_WIDTH, INPUT_HEIGHT))\n",
        "img = np.transpose(img, (2, 0, 1)) if INPUT_CHANNELS == 3 else img[np.newaxis]\n",
        "img_t = tf.convert_to_tensor(img)[np.newaxis, ...] / 255.0  # must be [BATCH_SIZE x CHANNELS x HEIGHT x WIDTH]\n",
        "\n",
        "dec_out, seg_out = model(img_t)\n",
        "img_score = tf.keras.activations.sigmoid(dec_out)\n",
        "print(img_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fdyrWmC88aW"
      },
      "source": [
        "## dataset.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-XRDFkS89PZ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.ndimage.morphology import distance_transform_edt\n",
        "from scipy.signal import convolve2d\n",
        "from config import Config\n",
        "\n",
        "\n",
        "class Dataset(tf.data.Dataset):\n",
        "    def _generator(self):\n",
        "        for index in range(self.len):\n",
        "            if self.counter >= self.len:\n",
        "                self.counter = 0\n",
        "                if self.frequency_sampling:\n",
        "                    sample_probability = 1 - (self.neg_retrieval_freq / np.max(self.neg_retrieval_freq))\n",
        "                    sample_probability = sample_probability - np.median(sample_probability) + 1\n",
        "                    sample_probability = sample_probability ** (np.log(len(sample_probability)) * 4)\n",
        "                    sample_probability = sample_probability / np.sum(sample_probability)\n",
        "\n",
        "                    # use replace=False for to get only unique values\n",
        "                    self.neg_imgs_permutation = np.random.choice(range(self.num_neg),\n",
        "                                                                 size=self.num_negatives_per_one_positive * self.num_pos,\n",
        "                                                                 p=sample_probability,\n",
        "                                                                 replace=False)\n",
        "                else:\n",
        "                    self.neg_imgs_permutation = np.random.permutation(self.num_neg)\n",
        "\n",
        "            if self.kind == 'TRAIN':\n",
        "                if index >= self.num_pos:\n",
        "                    ix = index % self.num_pos\n",
        "                    ix = self.neg_imgs_permutation[ix]\n",
        "                    item = self.neg_samples[ix]\n",
        "                    self.neg_retrieval_freq[ix] = self.neg_retrieval_freq[ix] + 1\n",
        "\n",
        "                else:\n",
        "                    ix = index\n",
        "                    item = self.pos_samples[ix]\n",
        "            else:\n",
        "                if index < self.num_neg:\n",
        "                    ix = index\n",
        "                    item = self.neg_samples[ix]\n",
        "                else:\n",
        "                    ix = index - self.num_neg\n",
        "                    item = self.pos_samples[ix]\n",
        "\n",
        "            image, seg_mask, seg_loss_mask, is_segmented, image_path, seg_mask_path, sample_name = item\n",
        "\n",
        "            if self.cfg.ON_DEMAND_READ:  # STEEL only so far\n",
        "                if image_path == -1 or seg_mask_path == -1:\n",
        "                    raise Exception('For ON_DEMAND_READ image and seg_mask paths must be set in read_contents')\n",
        "                img = self.read_img_resize(image_path, self.grayscale, self.image_size)\n",
        "                if seg_mask_path is None:  # good sample\n",
        "                    seg_mask = np.zeros_like(img)\n",
        "                elif isinstance(seg_mask_path, list):\n",
        "                    seg_mask = self.rle_to_mask(seg_mask_path, self.image_size)\n",
        "                else:\n",
        "                    seg_mask, _ = self.self.read_label_resize(seg_mask_path, self.image_size)\n",
        "\n",
        "                if np.max(seg_mask) == np.min(seg_mask):  # good sample\n",
        "                    seg_loss_mask = np.ones_like(seg_mask)\n",
        "                else:\n",
        "                    seg_loss_mask = self.distance_transform(seg_mask, self.cfg.WEIGHTED_SEG_LOSS_MAX, self.cfg.WEIGHTED_SEG_LOSS_P)\n",
        "\n",
        "                image = self.to_tensor(img)\n",
        "                seg_mask = self.to_tensor(self.downsize(seg_mask))\n",
        "                seg_loss_mask = self.to_tensor(self.downsize(seg_loss_mask))\n",
        "\n",
        "            self.counter = self.counter + 1\n",
        "\n",
        "            yield image, seg_mask, seg_loss_mask, is_segmented,sample_name\n",
        "\n",
        "           　\n",
        "\n",
        "    def __new__(cls, path: str, cfg: Config, kind: str):\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            cls._generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.bool),\n",
        "                tf.TensorSpec(shape=(), dtype=tf.string)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def __init__(self, path: str, cfg: Config, kind: str):\n",
        "        super(Dataset, self).__init__()\n",
        "        self.path: str = path\n",
        "        self.cfg: Config = cfg\n",
        "        self.kind: str = kind\n",
        "        self.image_size: (int, int) = (self.cfg.INPUT_WIDTH, self.cfg.INPUT_HEIGHT)\n",
        "        self.grayscale: bool = self.cfg.INPUT_CHANNELS == 1\n",
        "\n",
        "        self.num_negatives_per_one_positive: int = 1\n",
        "        self.frequency_sampling: bool = self.cfg.FREQUENCY_SAMPLING and self.kind == 'TRAIN'\n",
        "\n",
        "   \n",
        "   1def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def read_contents(self):\n",
        "        pass\n",
        "\n",
        "    def read_img_resize(self, path, grayscale, resize_dim) -> np.ndarray:\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE if grayscale else cv2.IMREAD_COLOR)\n",
        "        if resize_dim is not None:\n",
        "            img = cv2.resize(img, dsize=resize_dim)\n",
        "        return np.array(img, dtype=np.float32) / 255.0\n",
        "\n",
        "    def read_label_resize(self, path, resize_dim, dilate=None) -> (np.ndarray, bool):\n",
        "        lbl = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        if dilate is not None and dilate > 1:\n",
        "            lbl = cv2.dilate(lbl, np.ones((dilate, dilate)))\n",
        "        if resize_dim is not None:\n",
        "            lbl = cv2.resize(lbl, dsize=resize_dim)\n",
        "        return np.array((lbl / 255.0), dtype=np.float32), np.max(lbl) > 0\n",
        "\n",
        "    # Tensorflowへ書き換え\n",
        "    def to_tensor(self, x):\n",
        "        if x.dtype != np.float32:\n",
        "            x = (x / 255.0).astype(np.float32)\n",
        "\n",
        "        if len(x.shape) == 3:\n",
        "            x = np.transpose(x, axes=(2, 0, 1))\n",
        "        else:\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "\n",
        "        x = tf.convert_to_tensor(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def distance_transform(self, mask: np.ndarray, max_val: float, p: float) -> np.ndarray:\n",
        "        h, w = mask.shape[:2]\n",
        "        dst_trf = np.zeros((h, w))\n",
        "        \n",
        "        num_labels, labels = cv2.connectedComponents((mask * 255.0).astype(np.uint8), connectivity=8)\n",
        "        for idx in range(1, num_labels):\n",
        "            mask_roi= np.zeros((h, w))\n",
        "            k = labels == idx\n",
        "            mask_roi[k] = 255\n",
        "            dst_trf_roi = distance_transform_edt(mask_roi)\n",
        "            if dst_trf_roi.max() > 0:\n",
        "                dst_trf_roi = (dst_trf_roi / dst_trf_roi.max())\n",
        "                dst_trf_roi = (dst_trf_roi ** p) * max_val\n",
        "            dst_trf += dst_trf_roi\n",
        "\n",
        "        dst_trf[mask == 0] = 1\n",
        "        return np.array(dst_trf, dtype=np.float32)\n",
        "\n",
        "    # Tensorflowへ書き換え\n",
        "    def downsize(self, image: np.ndarray, downsize_factor: int = 8) -> np.ndarray:\n",
        "        img_t = tf.convert_to_tensor(np.expand_dims(image, 0 if len(image.shape) == 3 else (0, 1)).astype(np.float32))\n",
        "        img_t = tf.pad(img_t, [[0, 0], [downsize_factor, downsize_factor], [downsize_factor, downsize_factor], [0, 0]], mode='REFLECT')\n",
        "        image_np = tf.nn.avg_pool(img_t, ksize=2 * downsize_factor + 1, strides=downsize_factor, padding='VALID')\n",
        "        return image_np[0] if len(image.shape) == 3 else image_np[0, 0]\n",
        "\n",
        "\n",
        "    def rle_to_mask(self, rle, image_size):\n",
        "        if len(rle) % 2 != 0:\n",
        "            raise Exception('Suspicious')\n",
        "\n",
        "        w, h = image_size\n",
        "        mask_label = np.zeros(w * h, dtype=np.float32)\n",
        "\n",
        "        positions = rle[0::2]\n",
        "        length = rle[1::2]\n",
        "        for pos, le in zip(positions, length):\n",
        "            mask_label[pos - 1:pos + le - 1] = 1\n",
        "        mask = np.reshape(mask_label, (h, w), order='F').astype(np.uint8)\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzF3OVDyFveG"
      },
      "source": [
        "## train_net.py（x）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQWKYeLLd53e"
      },
      "outputs": [],
      "source": [
        "#Colabではうまく動かない\n",
        "#from end2end import End2End\n",
        "import argparse\n",
        "from config import Config\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--GPU', type=int, required=False, help=\"ID of GPU used for training/evaluation.\")\n",
        "    parser.add_argument('--RUN_NAME', type=str, required=False, help=\"Name of the run, used as directory name for storing results.\")\n",
        "    parser.add_argument('--DATASET', type=str, required=False, help=\"Which dataset to use.\")\n",
        "    parser.add_argument('--DATASET_PATH', type=str, required=False, help=\"Path to the dataset.\")\n",
        "\n",
        "    parser.add_argument('--EPOCHS', type=int, required=False, help=\"Number of training epochs.\")\n",
        "\n",
        "    parser.add_argument('--LEARNING_RATE', type=float, required=False, help=\"Learning rate.\")\n",
        "    parser.add_argument('--DELTA_CLS_LOSS', type=float, required=False, help=\"Weight delta for classification loss.\")\n",
        "\n",
        "    parser.add_argument('--BATCH_SIZE', type=int, required=False, help=\"Batch size for training.\")\n",
        "\n",
        "    parser.add_argument('--WEIGHTED_SEG_LOSS', type=str2bool, required=False, help=\"Whether to use weighted segmentation loss.\")\n",
        "    parser.add_argument('--WEIGHTED_SEG_LOSS_P', type=float, required=False, default=None, help=\"Degree of polynomial for weighted segmentation loss.\")\n",
        "    parser.add_argument('--WEIGHTED_SEG_LOSS_MAX', type=float, required=False, default=None, help=\"Scaling factor for weighted segmentation loss.\")\n",
        "    parser.add_argument('--DYN_BALANCED_LOSS', type=str2bool, required=False, help=\"Whether to use dynamically balanced loss.\")\n",
        "    parser.add_argument('--GRADIENT_ADJUSTMENT', type=str2bool, required=False, help=\"Whether to use gradient adjustment.\")\n",
        "    parser.add_argument('--FREQUENCY_SAMPLING', type=str2bool, required=False, help=\"Whether to use frequency-of-use based sampling.\")\n",
        "\n",
        "    parser.add_argument('--DILATE', type=int, required=False, default=None, help=\"Size of dilation kernel for labels\")\n",
        "\n",
        "    parser.add_argument('--FOLD', type=int, default=None, help=\"Which fold (KSDD) or class (DAGM) to train.\")\n",
        "    parser.add_argument('--TRAIN_NUM', type=int, default=None, help=\"Number of positive training samples for KSDD or STEEL.\")\n",
        "    parser.add_argument('--NUM_SEGMENTED', type=int, required=False, default=None, help=\"Number of segmented positive  samples.\")\n",
        "    parser.add_argument('--RESULTS_PATH', type=str, default=None, help=\"Directory to which results are saved.\")\n",
        "\n",
        "    parser.add_argument('--VALIDATE', type=str2bool, default=None, help=\"Whether to validate during training.\")\n",
        "    parser.add_argument('--VALIDATE_ON_TEST', type=str2bool, default=None, help=\"Whether to validate on test set.\")\n",
        "    parser.add_argument('--VALIDATION_N_EPOCHS', type=int, default=None, help=\"Number of epochs between consecutive validation runs.\")\n",
        "    parser.add_argument('--USE_BEST_MODEL', type=str2bool, default=None, help=\"Whether to use the best model according to validation metrics for evaluation.\")\n",
        "\n",
        "    parser.add_argument('--ON_DEMAND_READ', type=str2bool, default=None, help=\"Whether to use on-demand read of data from disk instead of storing it in memory.\")\n",
        "    parser.add_argument('--REPRODUCIBLE_RUN', type=str2bool, default=None, help=\"Whether to fix seeds and disable CUDA benchmark mode.\")\n",
        "\n",
        "    parser.add_argument('--MEMORY_FIT', type=int, default=None, help=\"How many images can be fitted in GPU memory.\")\n",
        "    parser.add_argument('--SAVE_IMAGES', type=str2bool, default=None, help=\"Save test images or not.\")\n",
        "    #parser.add_argument(\"-f\", required=False)\n",
        "\n",
        "    #args = parser.parse_args()\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    return args\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parse_args()\n",
        "\n",
        "    configuration = Config()\n",
        "    configuration.merge_from_args(args)\n",
        "    configuration.init_extra()\n",
        "\n",
        "    end2end = End2End(cfg=configuration)\n",
        "    end2end.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CPkaZoao8vFk"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1rNogHlKU_0KPIvPEyTLC_O2evlpyrFHn",
      "authorship_tag": "ABX9TyMIn5fsFLkej4dPW2fQDMDU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}